# üöÄ Trading Analyzer Improvement Roadmap

This document outlines potential improvements for both the modular and monolithic versions of the Trading Signal Analyzer, organized by category and priority.

---

## üèéÔ∏è **CATEGORY 1: PERFORMANCE & SPEED**

### **Priority: HIGH**

#### 1.1 Parallel Multi-Ticker Analysis
**Current Issue:** Tickers analyzed sequentially (1 by 1)
**Improvement:** Use multiprocessing pool for concurrent analysis

```python
from concurrent.futures import ProcessPoolExecutor, as_completed

def scan_tickers_parallel(tickers, max_workers=5):
    """Analyze multiple tickers concurrently"""
    results = []
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        future_to_ticker = {
            executor.submit(analyze_ticker, ticker): ticker
            for ticker in tickers
        }

        for future in as_completed(future_to_ticker):
            result = future.result()
            if result:
                results.append(result)

    return results
```

**Benefits:**
- 5-10x faster for market-wide scans
- Better CPU utilization
- Configurable worker count based on system

**Implementation:** 2-3 hours
**Files:** `pressurecooker_enhanced.py`, `momentum.py`, `darkflow.py`

---

#### 1.2 Smart Candidate Pre-Filtering
**Current Issue:** Full deep analysis on all candidates
**Improvement:** Quick metrics check before expensive analysis

```python
def quick_filter(ticker):
    """Fast pre-filter using basic metrics only"""
    try:
        stock = yf.Ticker(ticker)
        info = stock.info

        # Quick checks (no historical data needed)
        price = info.get('currentPrice', 0)
        volume = info.get('volume', 0)
        float_shares = info.get('floatShares', 999_999_999)

        # Instant rejection criteria
        if price < 1.0 or price > 100:
            return False
        if volume < 100_000:
            return False
        if float_shares > 50_000_000:
            return False

        return True
    except:
        return False
```

**Benefits:**
- Skip expensive yfinance.history() calls for obvious rejections
- Reduce API rate limit hits
- 30-40% faster overall scan time

**Implementation:** 1-2 hours

---

#### 1.3 Intelligent Caching with Invalidation
**Current Issue:** Cache doesn't track data freshness for intraday changes
**Improvement:** TTL-based cache with market hours awareness

```python
class SmartCache:
    def __init__(self):
        self.cache = {}
        self.timestamps = {}

    def get_ttl(self, data_type):
        """Dynamic TTL based on market hours"""
        now = datetime.now()
        market_open = now.replace(hour=9, minute=30)
        market_close = now.replace(hour=16, minute=0)

        is_market_hours = market_open <= now <= market_close

        if data_type == 'price':
            return 60 if is_market_hours else 3600  # 1 min vs 1 hour
        elif data_type == 'technical':
            return 300 if is_market_hours else 7200  # 5 min vs 2 hours
        else:
            return 900  # 15 min default
```

**Benefits:**
- Fresh data during market hours
- Less API usage after hours
- Automatic cache invalidation

**Implementation:** 2-3 hours

---

#### 1.4 Batch API Requests
**Current Issue:** Individual API calls for each ticker
**Improvement:** Batch requests where possible

```python
def batch_get_quotes(tickers, batch_size=100):
    """Get quotes in batches to reduce API calls"""
    quotes = {}

    for i in range(0, len(tickers), batch_size):
        batch = tickers[i:i+batch_size]
        batch_str = ' '.join(batch)

        # yfinance supports space-separated tickers
        data = yf.download(batch_str, period='1d', group_by='ticker')

        for ticker in batch:
            if ticker in data:
                quotes[ticker] = data[ticker]

    return quotes
```

**Benefits:**
- 50-70% fewer API calls
- Faster market-wide scans
- Less rate limiting issues

**Implementation:** 3-4 hours

---

## üìä **CATEGORY 2: BETTER INDICATORS & ANALYSIS**

### **Priority: MEDIUM-HIGH**

#### 2.1 Volume Profile (VWAP Anchored)
**What:** Volume distribution at price levels
**Why:** Shows institutional accumulation/distribution zones

```python
def calculate_volume_profile(hist, num_bins=20):
    """Calculate volume profile (volume at price)"""
    price_min = hist['Low'].min()
    price_max = hist['High'].max()
    bins = np.linspace(price_min, price_max, num_bins)

    volume_at_price = np.zeros(num_bins - 1)

    for i, row in hist.iterrows():
        # Find which bins this candle touches
        low_bin = np.digitize(row['Low'], bins) - 1
        high_bin = np.digitize(row['High'], bins) - 1

        # Distribute volume across bins
        for bin_idx in range(low_bin, high_bin + 1):
            if 0 <= bin_idx < len(volume_at_price):
                volume_at_price[bin_idx] += row['Volume']

    # Find Point of Control (POC) - highest volume price
    poc_idx = np.argmax(volume_at_price)
    poc_price = bins[poc_idx]

    return {
        'poc_price': poc_price,
        'volume_distribution': volume_at_price,
        'bins': bins
    }
```

**Scoring Integration:**
- +5 points if current price near POC (institutions accumulated there)
- +8 points if breaking above POC with volume
- -3 points if far below POC (resistance overhead)

**Implementation:** 4-5 hours

---

#### 2.2 Support/Resistance Level Detection
**What:** Algorithmic S/R level identification
**Why:** Better entry/exit timing, avoid resistance zones

```python
def find_support_resistance(hist, window=5, threshold=0.02):
    """Find support and resistance levels using pivot points"""
    highs = hist['High'].values
    lows = hist['Low'].values
    closes = hist['Close'].values

    resistance_levels = []
    support_levels = []

    # Find local maxima (resistance)
    for i in range(window, len(highs) - window):
        if highs[i] == max(highs[i-window:i+window+1]):
            resistance_levels.append(highs[i])

    # Find local minima (support)
    for i in range(window, len(lows) - window):
        if lows[i] == min(lows[i-window:i+window+1]):
            support_levels.append(lows[i])

    # Cluster nearby levels
    resistance_levels = cluster_levels(resistance_levels, threshold)
    support_levels = cluster_levels(support_levels, threshold)

    current_price = closes[-1]

    # Find nearest levels
    nearest_resistance = min([r for r in resistance_levels if r > current_price],
                            default=None)
    nearest_support = max([s for s in support_levels if s < current_price],
                         default=None)

    return {
        'resistance_levels': sorted(resistance_levels, reverse=True),
        'support_levels': sorted(support_levels, reverse=True),
        'nearest_resistance': nearest_resistance,
        'nearest_support': nearest_support,
        'distance_to_resistance': ((nearest_resistance - current_price) / current_price * 100)
                                  if nearest_resistance else None,
        'distance_to_support': ((current_price - nearest_support) / current_price * 100)
                               if nearest_support else None
    }
```

**Scoring Integration:**
- +10 points if breaking resistance with volume
- +5 points if bouncing off support
- -5 points if approaching resistance without catalyst

**Implementation:** 5-6 hours

---

#### 2.3 Chart Pattern Recognition
**What:** Detect bull flags, cup & handle, ascending triangles
**Why:** High-probability continuation patterns

```python
def detect_bull_flag(hist, lookback=20):
    """Detect bull flag pattern"""
    if len(hist) < lookback:
        return False, {}

    recent = hist.tail(lookback)
    prices = recent['Close'].values
    volumes = recent['Volume'].values

    # Bull flag characteristics:
    # 1. Strong move up (pole)
    # 2. Consolidation (flag) with declining volume
    # 3. Flag trending slightly down or sideways

    # Find the pole (biggest move in first half)
    half = lookback // 2
    pole_start = prices[0]
    pole_end = max(prices[:half])
    pole_gain = (pole_end - pole_start) / pole_start

    if pole_gain < 0.15:  # Need at least 15% move for pole
        return False, {}

    # Check flag (second half)
    flag_prices = prices[half:]
    flag_volumes = volumes[half:]

    # Volume should decline in flag
    early_vol = np.mean(volumes[:half])
    flag_vol = np.mean(flag_volumes)
    volume_decline = flag_vol < early_vol * 0.7

    # Flag should be consolidating (low volatility)
    flag_range = (max(flag_prices) - min(flag_prices)) / pole_gain
    tight_flag = flag_range < 0.5  # Flag range < 50% of pole

    # Trend of flag (slight downward is ideal)
    flag_slope = np.polyfit(range(len(flag_prices)), flag_prices, 1)[0]
    downward_flag = flag_slope <= 0

    is_bull_flag = volume_decline and tight_flag

    return is_bull_flag, {
        'pole_gain_pct': pole_gain * 100,
        'flag_tightness': flag_range,
        'volume_declining': volume_decline,
        'pattern_quality': 'STRONG' if (volume_decline and tight_flag and downward_flag) else 'WEAK'
    }
```

**Patterns to Detect:**
- Bull Flag (+15 points if strong)
- Cup & Handle (+12 points)
- Ascending Triangle (+10 points)
- Double Bottom (+8 points)

**Implementation:** 8-10 hours for all patterns

---

#### 2.4 Better Short Interest Data (FINRA)
**Current Issue:** yfinance short data is often stale/missing
**Improvement:** Direct FINRA short interest data

```python
def get_finra_short_interest(ticker):
    """Get latest short interest from FINRA"""
    import requests
    from bs4 import BeautifulSoup

    # FINRA publishes short interest data
    # Updated twice monthly (settlement dates)

    url = f"http://www.finra.org/finra-data/browse-catalog/short-sale-volume-data"

    # This would require web scraping or API if available
    # Returns more accurate short interest data

    return {
        'short_interest_shares': 0,  # From FINRA
        'short_percent_float': 0,    # Calculated
        'days_to_cover': 0,          # Updated
        'data_date': None,           # When data was published
        'data_freshness_days': 0     # How old the data is
    }
```

**Benefits:**
- More accurate short interest
- Know data freshness
- Better squeeze detection

**Implementation:** 6-8 hours (includes web scraping)

---

#### 2.5 Failure to Deliver (FTD) Detection
**What:** Track FTD data from SEC
**Why:** High FTDs can indicate naked shorting ‚Üí squeeze potential

```python
def get_ftd_data(ticker, days=30):
    """Get Failure to Deliver data from SEC"""
    # SEC publishes FTD data for all equity securities
    # Updated daily with T+2 lag

    # URL: https://www.sec.gov/data/foiadocsfailsdatahtm

    return {
        'total_ftd_shares': 0,
        'ftd_as_pct_float': 0,
        'consecutive_ftd_days': 0,
        'ftd_trend': 'increasing',  # or 'decreasing', 'stable'
    }
```

**Scoring:**
- +10 points if FTD > 1% of float
- +15 points if FTD > 5% of float (extreme)
- +5 points if FTD increasing

**Implementation:** 6-8 hours

---

## üé® **CATEGORY 3: USER EXPERIENCE**

### **Priority: MEDIUM**

#### 3.1 Interactive TUI (Text User Interface)
**Current Issue:** Menu-driven interface requires many keystrokes
**Improvement:** Rich TUI with live updates

```python
from rich.console import Console
from rich.live import Live
from rich.table import Table
from rich.layout import Layout

def create_live_dashboard():
    """Create live-updating dashboard"""
    layout = Layout()

    layout.split_column(
        Layout(name="header", size=3),
        Layout(name="body"),
        Layout(name="footer", size=3)
    )

    layout["body"].split_row(
        Layout(name="results"),
        Layout(name="details")
    )

    with Live(layout, refresh_per_second=1):
        while True:
            # Update results table
            results_table = create_results_table()
            layout["results"].update(results_table)

            # Update details panel
            layout["details"].update(selected_details)

            # Handle keyboard input
            key = get_key_press()
            if key == 'q':
                break
```

**Features:**
- Live updating results
- Keyboard navigation (arrow keys, vim keys)
- Split-pane view (list + details)
- Color coding (green/red)
- Progress bars for scans

**Libraries:** `rich`, `blessed`, or `textual`
**Implementation:** 15-20 hours

---

#### 3.2 ASCII Charts in Terminal
**What:** Visual price/volume charts in terminal
**Why:** Quickly see setup without external tools

```python
from plotille import Figure, plot

def display_ascii_chart(hist):
    """Display price chart in terminal"""
    fig = Figure()
    fig.width = 80
    fig.height = 20
    fig.color_mode = 'byte'

    dates = list(range(len(hist)))
    prices = hist['Close'].values
    volumes = hist['Volume'].values

    # Price line
    fig.plot(dates, prices, lc='green', label='Price')

    # Volume bars (scaled)
    vol_scaled = volumes / max(volumes) * max(prices) * 0.3
    fig.plot(dates, vol_scaled, lc='blue', label='Volume')

    print(fig.show(legend=True))

    # Add annotations for key levels
    print(f"\n  Support: ${nearest_support:.2f}  |  Resistance: ${nearest_resistance:.2f}")
    print(f"  RSI: {rsi:.1f}  |  Volume: {rel_vol:.1f}x")
```

**Libraries:** `plotille`, `termplotlib`, or `asciichartpy`
**Implementation:** 3-4 hours

---

#### 3.3 Export to CSV/Excel/PDF
**What:** Export scan results in multiple formats
**Why:** Further analysis in Excel, archiving, sharing

```python
def export_results(results, format='csv', filename=None):
    """Export results to file"""
    if not filename:
        filename = f"scan_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    if format == 'csv':
        df = pd.DataFrame([r.__dict__ for r in results])
        df.to_csv(f"{filename}.csv", index=False)

    elif format == 'excel':
        df = pd.DataFrame([r.__dict__ for r in results])
        with pd.ExcelWriter(f"{filename}.xlsx", engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='Results', index=False)

            # Add formatting
            workbook = writer.book
            worksheet = writer.sheets['Results']

            # Conditional formatting for scores
            from openpyxl.formatting.rule import ColorScaleRule
            worksheet.conditional_formatting.add('G2:G100',
                ColorScaleRule(start_type='min', start_color='FF0000',
                              end_type='max', end_color='00FF00'))

    elif format == 'pdf':
        from fpdf import FPDF

        pdf = FPDF()
        pdf.add_page()
        pdf.set_font('Arial', 'B', 16)
        pdf.cell(0, 10, 'Pressure Cooker Scan Results', ln=True)
        pdf.set_font('Arial', '', 10)

        for r in results:
            pdf.cell(0, 5, f"{r.ticker}: {r.score}/100 - {r.setup_quality}", ln=True)

        pdf.output(f"{filename}.pdf")
```

**Implementation:** 4-5 hours

---

#### 3.4 Saved Scan Profiles/Presets
**What:** Save common scan configurations
**Why:** Quick re-runs of favorite scans

```python
# ~/.trading_analyzer_presets.json
{
  "presets": {
    "aggressive_squeeze": {
      "scanner": "pressure_cooker",
      "filters": {
        "max_float": 3000000,
        "min_short_percent": 25,
        "min_rel_vol": 7.0,
        "price_range": [3.0, 15.0]
      },
      "min_score": 75
    },
    "conservative_momentum": {
      "scanner": "momentum",
      "filters": {
        "min_change": 15.0,
        "min_rel_vol": 5.0,
        "max_float": 20000000
      },
      "min_score": 70
    }
  }
}
```

**Usage:**
```bash
$ python -m trading_analyzer --preset aggressive_squeeze
$ python -m trading_analyzer --preset conservative_momentum
```

**Implementation:** 3-4 hours

---

#### 3.5 Command-Line Arguments for Automation
**What:** Run scans from command line without interactive menu
**Why:** Automation, cron jobs, scripting

```python
import argparse

parser = argparse.ArgumentParser(description='Trading Signal Analyzer')
parser.add_argument('--scanner', choices=['momentum', 'darkflow', 'pressure_cooker'],
                   help='Scanner type')
parser.add_argument('--market', default='1', help='Market choice (1-5)')
parser.add_argument('--mode', choices=['quick', 'smart', 'deep'], default='smart',
                   help='Scan mode')
parser.add_argument('--min-score', type=int, default=50, help='Minimum score')
parser.add_argument('--export', choices=['csv', 'excel', 'pdf'], help='Export format')
parser.add_argument('--output', help='Output filename')
parser.add_argument('--preset', help='Load saved preset')
parser.add_argument('--ticker', help='Analyze specific ticker')

args = parser.parse_args()
```

**Examples:**
```bash
# Quick squeeze scan, export to CSV
$ python -m trading_analyzer --scanner pressure_cooker --mode quick --export csv

# Analyze specific ticker
$ python -m trading_analyzer --scanner pressure_cooker --ticker AAPL

# Load preset and export
$ python -m trading_analyzer --preset aggressive_squeeze --export excel --output daily_scan
```

**Implementation:** 2-3 hours

---

## üîç **CATEGORY 4: DATA QUALITY & RELIABILITY**

### **Priority: MEDIUM**

#### 4.1 Multiple Data Source Fallbacks
**Current Issue:** Single dependency on yfinance
**Improvement:** Fallback to alternative sources

```python
class MultiSourceDataProvider:
    """Try multiple data sources with fallback"""

    def __init__(self):
        self.sources = [
            YFinanceProvider(),
            PolygonProvider(),  # If user has API key
            AlphaVantageProvider(),
            TwelveDataProvider()
        ]

    def get_ticker_data(self, ticker, period='6mo'):
        """Try sources in order until one succeeds"""
        for source in self.sources:
            try:
                data = source.fetch_data(ticker, period)
                if data and not data.empty:
                    return data, source.name
            except Exception as e:
                continue

        return None, None

    def get_realtime_quote(self, ticker):
        """Get most recent quote with fallback"""
        # Try sources that have real-time data
        for source in [s for s in self.sources if s.has_realtime]:
            try:
                quote = source.get_quote(ticker)
                if quote:
                    return quote
            except:
                continue

        return None
```

**Benefits:**
- More reliable scans
- Graceful degradation
- Better coverage (some tickers only on certain sources)

**Implementation:** 6-8 hours

---

#### 4.2 Data Validation & Sanity Checks
**What:** Verify data quality before analysis
**Why:** Avoid bad trades from bad data

```python
def validate_stock_data(ticker, hist, info):
    """Validate data quality and flag issues"""
    issues = []

    # Check for missing data
    if hist.empty:
        issues.append(("CRITICAL", "No historical data"))

    # Check for data gaps
    trading_days = len(hist)
    expected_days = 20  # For 1mo period
    if trading_days < expected_days * 0.7:
        issues.append(("WARNING", f"Data gaps detected ({trading_days}/{expected_days} days)"))

    # Check for price anomalies
    price_changes = hist['Close'].pct_change()
    extreme_moves = abs(price_changes) > 0.50  # >50% single day
    if extreme_moves.any():
        issues.append(("WARNING", "Extreme price movements detected (possible split/error)"))

    # Check volume consistency
    if (hist['Volume'] == 0).sum() > 3:
        issues.append(("WARNING", "Multiple zero-volume days"))

    # Verify info data
    if not info.get('floatShares'):
        issues.append(("INFO", "Float data missing"))

    if not info.get('shortPercentOfFloat'):
        issues.append(("INFO", "Short interest data missing"))

    # Check for stale data
    last_date = hist.index[-1]
    days_old = (datetime.now() - last_date).days
    if days_old > 5:
        issues.append(("WARNING", f"Data is {days_old} days old"))

    return {
        'is_valid': len([i for i in issues if i[0] == 'CRITICAL']) == 0,
        'issues': issues,
        'data_quality_score': calculate_quality_score(issues)
    }
```

**Display:**
```
‚ö†Ô∏è  DATA QUALITY ISSUES for XYZ:
   ‚Ä¢ WARNING: Data gaps detected (12/20 days)
   ‚Ä¢ INFO: Short interest data missing

   Data Quality Score: 65/100

   Continue with analysis? (y/n)
```

**Implementation:** 3-4 hours

---

## üß† **CATEGORY 5: ADVANCED ANALYSIS**

### **Priority: LOW-MEDIUM**

#### 5.1 Backtesting Framework
**What:** Test scoring on historical squeeze events
**Why:** Validate scoring accuracy, optimize weights

```python
class PressureCookerBacktest:
    """Backtest squeeze detection on historical squeezes"""

    def __init__(self):
        # Historical squeezes to test against
        self.known_squeezes = [
            {'ticker': 'GME', 'date': '2021-01-27', 'peak_gain': 1625},
            {'ticker': 'AMC', 'date': '2021-06-02', 'peak_gain': 380},
            {'ticker': 'SPRT', 'date': '2021-08-27', 'peak_gain': 850},
            # ... more historical squeezes
        ]

    def test_scanner(self, scanner, lookback_days=10):
        """Test if scanner would have caught these setups"""
        results = {
            'true_positives': 0,   # Caught squeeze that happened
            'false_negatives': 0,  # Missed squeeze that happened
            'scores': []
        }

        for squeeze in self.known_squeezes:
            # Analyze stock X days before squeeze
            test_date = datetime.strptime(squeeze['date'], '%Y-%m-%d')
            test_date = test_date - timedelta(days=lookback_days)

            # Get historical analysis
            analysis = scanner.analyze_ticker_at_date(
                squeeze['ticker'],
                test_date
            )

            if analysis:
                if analysis['score'] >= 70:
                    results['true_positives'] += 1
                    results['scores'].append(analysis['score'])
                else:
                    results['false_negatives'] += 1

        # Calculate metrics
        accuracy = results['true_positives'] / len(self.known_squeezes)
        avg_score = np.mean(results['scores']) if results['scores'] else 0

        return {
            'accuracy': accuracy,
            'true_positives': results['true_positives'],
            'false_negatives': results['false_negatives'],
            'average_score': avg_score,
            'total_tested': len(self.known_squeezes)
        }
```

**Benefits:**
- Validate scoring algorithm
- Optimize weight distributions
- Build confidence in system

**Implementation:** 10-12 hours

---

#### 5.2 Similar Setup Finder
**What:** Find stocks with similar technical patterns
**Why:** Pattern recognition, idea generation

```python
def find_similar_setups(reference_ticker, candidate_tickers, top_n=5):
    """Find stocks with similar technical setup"""

    # Get reference pattern
    ref_analysis = analyze_ticker(reference_ticker)
    if not ref_analysis:
        return []

    similarities = []

    for ticker in candidate_tickers:
        analysis = analyze_ticker(ticker)
        if not analysis:
            continue

        # Calculate similarity score
        similarity = 0

        # RSI similarity (¬±10 points)
        rsi_diff = abs(ref_analysis['rsi'] - analysis['rsi'])
        if rsi_diff < 10:
            similarity += 20

        # Volume pattern similarity
        vol_diff = abs(ref_analysis['rel_vol'] - analysis['rel_vol'])
        if vol_diff < 2.0:
            similarity += 15

        # Float similarity
        float_ratio = analysis['float_millions'] / ref_analysis['float_millions']
        if 0.5 < float_ratio < 2.0:
            similarity += 10

        # Short interest similarity
        short_diff = abs(ref_analysis['short_percent'] - analysis['short_percent'])
        if short_diff < 10:
            similarity += 15

        # Setup stage match
        if ref_analysis['setup_stage'] == analysis['setup_stage']:
            similarity += 20

        similarities.append({
            'ticker': ticker,
            'similarity_score': similarity,
            'analysis': analysis
        })

    # Sort by similarity
    similarities.sort(key=lambda x: x['similarity_score'], reverse=True)

    return similarities[:top_n]
```

**Usage:**
```
$ Enter reference ticker: GME
$ Finding stocks with similar setups...

Similar Setups to GME:
1. AMC - 85% similar
   ‚Ä¢ RSI: 32 (vs GME 28)
   ‚Ä¢ RelVol: 8.2x (vs GME 9.1x)
   ‚Ä¢ Setup Stage: READY (match)

2. BBBY - 72% similar
   ...
```

**Implementation:** 6-8 hours

---

## üìà **CATEGORY 6: REPORTING & TRACKING**

### **Priority: LOW**

#### 6.1 Daily Digest Emails
**What:** Automated scan results via email
**Why:** Stay informed without manual scanning

```python
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText

def send_daily_digest(results, user_email):
    """Send HTML email with scan results"""

    # Create HTML email
    html = f"""
    <html>
    <body>
        <h2>üî• Daily Pressure Cooker Scan Results</h2>
        <p>Found {len(results)} setups scoring 70+</p>

        <table border="1">
            <tr>
                <th>Ticker</th>
                <th>Score</th>
                <th>Float</th>
                <th>Short%</th>
                <th>Stage</th>
            </tr>
    """

    for r in results[:10]:  # Top 10
        html += f"""
            <tr>
                <td><b>{r.ticker}</b></td>
                <td>{r.score}/100</td>
                <td>{r.float_m:.1f}M</td>
                <td>{r.short_percent:.1f}%</td>
                <td>{r.setup_stage.upper()}</td>
            </tr>
        """

    html += """
        </table>
        <p><i>Generated by Trading Analyzer</i></p>
    </body>
    </html>
    """

    # Send email
    msg = MIMEMultipart('alternative')
    msg['Subject'] = f'Daily Scan: {len(results)} Setups Found'
    msg['From'] = 'scanner@tradinganalyzer.com'
    msg['To'] = user_email

    msg.attach(MIMEText(html, 'html'))

    # Use email settings from config
    server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)
    server.starttls()
    server.login(EMAIL_USER, EMAIL_PASS)
    server.send_message(msg)
    server.quit()
```

**Configuration in settings:**
```json
{
  "email_notifications": {
    "enabled": true,
    "email_address": "user@example.com",
    "daily_digest": {
      "enabled": true,
      "time": "16:30",
      "min_score": 70,
      "max_results": 10
    }
  }
}
```

**Implementation:** 4-5 hours

---

#### 6.2 Performance Tracking
**What:** Track success rate of setups over time
**Why:** Learn what works, improve scoring

```python
class PerformanceTracker:
    """Track performance of identified setups"""

    def __init__(self, db_path='performance.db'):
        self.db = sqlite3.connect(db_path)
        self.create_tables()

    def record_setup(self, ticker, score, entry_price, entry_date):
        """Record when we identified a setup"""
        self.db.execute("""
            INSERT INTO tracked_setups
            (ticker, score, entry_price, entry_date, status)
            VALUES (?, ?, ?, ?, 'ACTIVE')
        """, (ticker, score, entry_price, entry_date))
        self.db.commit()

    def update_performance(self, days_to_track=30):
        """Check performance of active setups"""
        active = self.db.execute("""
            SELECT id, ticker, entry_price, entry_date, score
            FROM tracked_setups
            WHERE status = 'ACTIVE'
            AND entry_date >= date('now', '-30 days')
        """).fetchall()

        for setup_id, ticker, entry_price, entry_date, score in active:
            # Get current price
            current_price = get_current_price(ticker)

            # Calculate gain/loss
            gain_pct = ((current_price - entry_price) / entry_price) * 100

            # Check if setup played out
            days_since = (datetime.now() - datetime.strptime(entry_date, '%Y-%m-%d')).days

            if gain_pct >= 20:  # Hit target
                status = 'WIN'
            elif gain_pct <= -10:  # Hit stop
                status = 'LOSS'
            elif days_since >= 30:  # Expired
                status = 'EXPIRED'
            else:
                status = 'ACTIVE'

            # Update database
            self.db.execute("""
                UPDATE tracked_setups
                SET current_price = ?, gain_pct = ?, status = ?
                WHERE id = ?
            """, (current_price, gain_pct, status, setup_id))

        self.db.commit()

    def get_statistics(self):
        """Get performance statistics"""
        stats = self.db.execute("""
            SELECT
                COUNT(*) as total,
                AVG(gain_pct) as avg_gain,
                SUM(CASE WHEN status = 'WIN' THEN 1 ELSE 0 END) as wins,
                SUM(CASE WHEN status = 'LOSS' THEN 1 ELSE 0 END) as losses,
                AVG(CASE WHEN status = 'WIN' THEN gain_pct END) as avg_win,
                AVG(CASE WHEN status = 'LOSS' THEN gain_pct END) as avg_loss
            FROM tracked_setups
            WHERE status IN ('WIN', 'LOSS', 'EXPIRED')
        """).fetchone()

        return {
            'total_setups': stats[0],
            'avg_gain': stats[1],
            'wins': stats[2],
            'losses': stats[3],
            'win_rate': (stats[2] / (stats[2] + stats[3])) * 100 if stats[2] + stats[3] > 0 else 0,
            'avg_win': stats[4],
            'avg_loss': stats[5],
            'profit_factor': abs(stats[4] / stats[5]) if stats[5] and stats[5] != 0 else 0
        }
```

**Display:**
```
üìä PERFORMANCE STATISTICS (Last 90 days)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Total Setups Tracked: 47
Win Rate: 64.3% (27 wins / 15 losses)
Average Gain: +12.3%

Breakdown by Score:
  90-100: 80% win rate (8 setups)
  80-89:  71% win rate (14 setups)
  70-79:  52% win rate (18 setups)
  60-69:  33% win rate (7 setups)

Best Performers:
  1. SPRT +347% (Score: 94)
  2. GME +182% (Score: 91)
  3. AMC +124% (Score: 88)

Conclusion: Setups scoring 80+ have 75% success rate
```

**Implementation:** 8-10 hours

---

## üéØ **PRIORITY IMPLEMENTATION ORDER**

### **Phase 1 (Quick Wins - 1-2 weeks)**
1. ‚úÖ Parallel multi-ticker analysis (5-10x speed boost)
2. ‚úÖ Smart candidate pre-filtering (30-40% faster)
3. ‚úÖ Command-line arguments for automation
4. ‚úÖ Export to CSV/Excel
5. ‚úÖ Data validation & sanity checks

### **Phase 2 (High-Value Features - 2-3 weeks)**
1. ‚úÖ Support/Resistance level detection
2. ‚úÖ Volume Profile (VWAP anchored)
3. ‚úÖ ASCII charts in terminal
4. ‚úÖ Saved scan profiles/presets
5. ‚úÖ Intelligent caching with market hours

### **Phase 3 (Advanced Features - 3-4 weeks)**
1. ‚úÖ Chart pattern recognition (bull flags, etc.)
2. ‚úÖ Better short interest data (FINRA)
3. ‚úÖ FTD detection
4. ‚úÖ Interactive TUI
5. ‚úÖ Multiple data source fallbacks

### **Phase 4 (Nice-to-Have - 4+ weeks)**
1. ‚úÖ Backtesting framework
2. ‚úÖ Similar setup finder
3. ‚úÖ Performance tracking
4. ‚úÖ Daily digest emails
5. ‚úÖ Machine learning scoring (advanced)

---

## üìù **SUMMARY**

This roadmap provides **25+ actionable improvements** ranging from quick performance wins to advanced analytical features.

**Recommended Starting Points:**
1. **Parallel processing** - Biggest immediate impact
2. **Pre-filtering** - Faster scans, less API abuse
3. **Support/Resistance** - Better entry/exit timing
4. **Export functionality** - User requested, easy to implement
5. **Command-line args** - Enable automation

**Time Estimate:**
- Phase 1: 40-50 hours
- Phase 2: 60-80 hours
- Phase 3: 80-100 hours
- Phase 4: 60-80 hours

**Total: 240-310 hours** (roughly 6-8 weeks full-time development)

Would you like me to start implementing any of these improvements?
